# Portfolio-Optimization
本專題利用現代投資組合理論、各種梯度下降演算法進行投資組合最佳化。透過投資組合優化以達**有效降低風險**，並**提高投資報酬**的效果。  
標的：美國股市  
歷史股價：python 爬蟲api  
最佳化模型：由matlab實作梯度下降及其延伸方法（Momentum、Adam、AdaGrad），最佳化報酬情況下找到相對權重  
 
## 均值－方差分析方法
提出者 : 馬柯維茨  
目的 :假設投資者是風險規避的，且追求期望效用最大化的情形下，可以有效降低風險，平衡報酬和風險。  
期望收益 :  $𝐸(𝑟)= \sum_{i=1}^{n}w_i*r_i$  
投資風險 : $𝜎^2(𝑟)= \sum_{i,j}^{n}𝑤_𝑖 𝑤_𝑗 𝑐𝑜𝑣(𝑟_𝑖,𝑟_𝑗)   ∀𝑖≠𝑗$  

## 夏普比率
投資人每多承擔一分風險，可以拿到幾分報酬；  
若為正值，代表基金報酬率高過波動風險；若為負值，代表風險大過於報酬率  
即投資回報與多冒風險的比例，這個比例越高，投資組合越佳。   

公式 :   $\dfrac{[𝐸(𝑟)−𝑟_𝑓]}{𝜎}$  

$E(Rp)$：投資組合預期報酬率  
$Rf$：無風險利率  
$σp$：投資組合的標準差  
註 :   
無風險利率 :  
指把資金投資於一個沒有任何風險的投資對象所能得到的收益率。一般會把這一收益率作為基本收益，再考慮可能出現的各種風險。  
公式 : 無風險收益率 = 資金時間價值（純利率）+通貨膨脹補償率   


## 梯度下降及ADAM
* 梯度(Gradient)  
令一個雙變數連續可微函數 z = f(x,y)，在三維直角坐標中，我們可以畫出一個連續平面。  

在平面上的任意一點(𝑥_0,𝑦_0)，我們都可以找到一個向量( fx(𝑥_0,𝑦_0) , fy(𝑥_0,𝑦_0) ) ， 使得該向量指向的方向𝑧_0上升速度最大，該向量為梯度  

* 梯度法(Gradient Method)  
利用梯度指向最速上升速度的性質，找到極值的近似值:  
	(𝑥_(𝑛+1),𝑦_(𝑛+1)) = (𝑥_𝑛,𝑦_𝑛) + η( fx(𝑥_𝑛,𝑦_𝑛) , fy(𝑥_𝑛,𝑦_𝑛) )  
推廣到任意有限量變數:  
   〖𝑊〗_(𝑛+1)  = 𝑊_𝑛+𝑔_𝑛  
梯度下降法利用小於零，下降到平面的相對極小值。   
* 各種梯度演算法的延伸  
Momentum  
我們可以利用一些方法加速疊代的速度
 momentum optimization定義一個 v 加入疊代式形成:  
vn+1 = vn+ gn  
Wn+1 = Wn + vn+1   
在更新權重的時候加入了之前的更新量，使得學習速度增加，不過舊的更新量會乘上作為衰減率，使得舊的更新量影響越來越小。如果當下梯度方向和歷史參數更新的方向一致，則會增強梯度方向的更新，若當下梯度方向和歷史參數更新的方向不一致，則會減少梯度方向的更新。然後每一次對梯度作方向微調。在梯度法後期經常會有之字震盪的情形，momentum可以輕鬆的抵銷相反的向量，減少擺盪問題。  
AdaGrad optimizer  
Wn+1 =Wn + (/Gn +  )gn   
我們在學習率除上Gn + ，Gn =k = 1ngk2是前面所有梯度平方的總和、是一個接近零的小數字，讓分母不為零。  
這樣可以選大一點，隨著疊代次數增加，Gn也會一起增加，使得學習率降低，可以讓學習初期學習速度快、提高效率，後期速度慢、提高精準度。  
RMSProp  
ADAM:  
momentum和AdaGrad 雖然加快了收斂速度，但都伴隨一些缺點：  
momentum 會偏移最速下降或上升的演算法，只不過利用移動速度快來解決繞一大圈的問題。   
AdaGrad 隨著梯度平方和越來越大，學習速度越來越小，可能還沒到極值就結束學習了。  
ADAM 結合了momentum 和 RMSProp(AdaGrad的進階演算法) 的優點，再加上偏離校正 1-β 消除了他們的缺點，使得學習速度比最原始的梯度法快好幾倍。  

### 輸出投資組合權重、波動率、回報率、夏普比率、標準差。
